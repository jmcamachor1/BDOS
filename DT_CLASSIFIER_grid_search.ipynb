{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=[ \"ner\",\"tagger\",'textcat'])\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import emoji as emoji\n",
    "import regex\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pandas import Panel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT_grid_search(hyper_comb,X_train,y_train,file_name,c_w):\n",
    "    start=time.time()\n",
    "    all_res_cv = []\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=1848773) # 5 fold\n",
    "    grid = ParameterGrid(hyper_comb) #all possible combination of parameters chosen\n",
    "    for hypers in tqdm(grid):\n",
    "        class_weight=hypers['class_weight'],\n",
    "        criterion=hypers['criterion']\n",
    "        max_depth=hypers['max_depth']\n",
    "        max_features=hypers['max_features']\n",
    "        max_leaf_nodes=hypers['max_leaf_nodes']\n",
    "        min_impurity_decrease=hypers['min_impurity_decrease']\n",
    "        min_impurity_split=hypers['min_impurity_split']\n",
    "        min_samples_leaf=hypers['min_samples_leaf']\n",
    "        min_samples_split=hypers['min_samples_split']\n",
    "        min_weight_fraction_leaf=hypers['min_weight_fraction_leaf']\n",
    "        presort=hypers['presort']\n",
    "        splitter=hypers['splitter']\n",
    "        val_f1_list = []\n",
    "        train_f1_list = []\n",
    "        val_recall_list=[]\n",
    "        train_recall_list=[]\n",
    "        val_accuracy_list=[]\n",
    "        train_accuracy_list=[]\n",
    "        val_pre_list=[]\n",
    "        train_pre_list=[]\n",
    "        \n",
    "\n",
    "        for train_index, test_index in (cv.split(X_train)): # cross validation\n",
    "            #start1=time.time()\n",
    "            #take 80% and 20%\n",
    "            X_train_cv, X_val, y_train_cv, y_val = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
    "            #fit the model with the chosen param1848773eters\n",
    "            model=tree.DecisionTreeClassifier(class_weight=c_w, criterion=criterion, max_depth=max_depth,\n",
    "                       max_features=max_features, max_leaf_nodes=max_leaf_nodes,\n",
    "                       min_impurity_decrease=min_impurity_decrease, min_impurity_split=min_impurity_split,\n",
    "                       min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split,\n",
    "                       min_weight_fraction_leaf=min_weight_fraction_leaf, presort=presort,\n",
    "                       random_state=1848773, splitter=splitter).fit(X_train_cv,y_train_cv)\n",
    "\n",
    "            # make prediction and compute F1\n",
    "            pred_train = model.predict(X_train_cv)\n",
    "            pred_val = model.predict(X_val)\n",
    "            #F1 SCORE\n",
    "            f1_train = f1_score(y_train_cv, pred_train, average='micro')\n",
    "            train_f1_list.append(f1_train)\n",
    "            f1_val = f1_score(y_val, pred_val, average='micro')\n",
    "            val_f1_list.append(f1_val)\n",
    "            #RECALL VALUES\n",
    "            recall_train=recall_score(y_train_cv,pred_train, average='micro')\n",
    "            train_recall_list.append(recall_train)\n",
    "            recall_val=recall_score(y_val,pred_val, average='micro')\n",
    "            val_recall_list.append(recall_val)\n",
    "            #ACCURACY\n",
    "            accuracy_train=accuracy_score(y_train_cv,pred_train)\n",
    "            train_accuracy_list.append(accuracy_train)\n",
    "            accuracy_val=accuracy_score(y_val,pred_val)\n",
    "            val_accuracy_list.append(accuracy_val)\n",
    "            #PRECISION\n",
    "            pre_train=precision_score(y_train_cv,pred_train, average='micro')\n",
    "            train_pre_list.append(pre_train)\n",
    "            pre_val=precision_score(y_val,pred_val, average='micro')\n",
    "            val_pre_list.append(pre_val)\n",
    "            #end1=time.time()\n",
    "\n",
    "        #print(end1-start1)\n",
    "        all_res_cv.append({ 'class_weight':class_weight,\n",
    "            'criterion':criterion,\n",
    "            'max_depth':max_depth,\n",
    "            'max_features':max_features,\n",
    "            'max_leaf_nodes':max_leaf_nodes,\n",
    "            'min_impurity_decrease':min_impurity_decrease,\n",
    "            'min_impurity_split':min_impurity_split,\n",
    "            'min_samples_leaf':min_samples_leaf,\n",
    "            'min_samples_split':min_samples_split,\n",
    "            'min_weight_fraction_leaf':min_weight_fraction_leaf,\n",
    "            'presort':presort,\n",
    "            'splitter':splitter,\n",
    "        'val_f1':np.mean(val_f1_list), \n",
    "        'train_f1':np.mean(train_f1_list),\n",
    "        'val_recall':np.mean(val_recall_list), \n",
    "        'train_recall':np.mean(train_recall_list),\n",
    "        'val_acc':np.mean(val_accuracy_list), \n",
    "        'train_acc':np.mean(train_accuracy_list),\n",
    "        'val_pre':np.mean(val_pre_list),\n",
    "        'train_pre':np.mean(train_pre_list)})\n",
    "        '''\n",
    "        print(({'class_weight':class_weight,\n",
    "            'criterion':criterion,\n",
    "            'max_depth':max_depth,\n",
    "            'max_features':max_features,\n",
    "            'max_leaf_nodes':max_leaf_nodes,\n",
    "            'min_impurity_decrease':min_impurity_decrease,\n",
    "            'min_impurity_split':min_impurity_split,\n",
    "            'min_samples_leaf':min_samples_leaf,\n",
    "            'min_samples_split':min_samples_split,\n",
    "            'min_weight_fraction_leaf':min_weight_fraction_leaf,\n",
    "            'presort':presort,\n",
    "            'splitter':splitter,\n",
    "            'val_f1':np.mean(val_f1_list), 'train_f1':np.mean(train_f1_list)}))\n",
    "            '''\n",
    "\n",
    "\n",
    "        # after the 5th iteration we make the mean of the collected errors to find a more reliable value for the error\n",
    "    end=time.time()-start\n",
    "    df = pd.DataFrame(all_res_cv) # collect all results in a dataframe\n",
    "    df.to_csv(file_name, index=False) # save dataframe in csv with results\n",
    "\n",
    "    return print('File created grid_search_cv')\n",
    "\n",
    "##combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('pre_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=list(train['prep_text'])\n",
    "sub[5115]='Err:509'\n",
    "train['prep_text']=sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "2\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 501.35it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm([8,2,2,3]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.36.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bow_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['prep_text']\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "X_train_=count_vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_comb={'class_weight':[None],\n",
    "            'criterion':['gini','entropy'],\n",
    "            'max_depth':[None],\n",
    "            'max_features':['auto','sqrt','log2',None],\n",
    "            'max_leaf_nodes':[None],\n",
    "            'min_impurity_decrease':[0],\n",
    "            'min_impurity_split':[1e-6,1e-7],\n",
    "            'min_samples_leaf':list(np.arange(0.1,0.5,0.15))+[2,3,4,5],\n",
    "            'min_samples_split':list(np.arange(0.1,1,0.15))+[2,3,4,5],\n",
    "            'min_weight_fraction_leaf':[0.0],\n",
    "            'presort':[False],\n",
    "            'splitter':['best','random']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#class_weight=None\n",
    "DT_grid_search(hyper_comb,X_train_,y,'DT_grid_search_cv_bow_none.csv',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7697355399425195"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(pd.read_csv('DT_grid_search_cv_bow_none.csv')['val_f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2240/2240 [27:58<00:00,  1.33it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created grid_search_cv\n"
     ]
    }
   ],
   "source": [
    "#class_weight=balanced\n",
    "DT_grid_search(hyper_comb,X_train_,y,'DT_grid_search_cv_bow_balanced.csv','balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7639563486893477"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(pd.read_csv('DT_grid_search_cv_bow_balanced.csv')['val_f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "train_tfidf_ = tfidf_transformer.fit_transform(X_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2240/2240 [36:25<00:00,  1.03it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created grid_search_cv\n"
     ]
    }
   ],
   "source": [
    "#class_weight=None\n",
    "DT_grid_search(hyper_comb,train_tfidf_,y,'DT_grid_search_cv_tfidf_none.csv',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7659265765489822"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(pd.read_csv('DT_grid_search_cv_tfidf_none.csv')['val_f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2240/2240 [54:33<00:00,  1.46s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created grid_search_cv\n"
     ]
    }
   ],
   "source": [
    "#class_weight='balanced'\n",
    "DT_grid_search(hyper_comb,train_tfidf_,y,'DT_grid_search_cv_tfidf_balanced.csv','balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7596208120255081"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(pd.read_csv('DT_grid_search_cv_tfidf_balanced.csv')['val_f1']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
