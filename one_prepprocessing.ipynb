{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=[ \"ner\",\"tagger\",'textcat'])\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import emoji as emoji\n",
    "import regex\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pandas import Panel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1\n",
    "def remove_stopwords(text):\n",
    "    text=text.lower()\n",
    "    tokens=[token for token in word_tokenize(text) if token not in stopwords.words('english')]\n",
    "    return ' '.join([w for w in tokens])\n",
    "##2\n",
    "def remove_Urls(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "## 3\n",
    "def clear_spaces(text):\n",
    "    text=text.replace(\"\\t\",\" \")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return re.sub(\"\\s\\s+\" , \" \",text).strip(\" \")\n",
    "##~4\n",
    "def remove_User(text):\n",
    "    return re.sub(r'@([^\\s:]+)', \"\", text)\n",
    "\n",
    "##5\n",
    "def removeDigits(text):\n",
    "        return ' '.join([w for w in word_tokenize(str(text)) if not re.search(r'\\d', w)])\n",
    "\n",
    "#6\n",
    "def correct_spellings(text):\n",
    "    spell = SpellChecker()\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "##7\n",
    "def clearPunct(text):\n",
    "        text=text.replace(\"#\",\"\").replace('¿','')\n",
    "        # a = re.sub(r'[^\\w\\s]',' ',text)\n",
    "        text=text.translate(str.maketrans('', '', string.punctuation))\n",
    "        return text.replace(\"_\",\"\")\n",
    "    \n",
    "\n",
    "##8\n",
    "def lemmatise(text):\n",
    "    doc=nlp(text.lower())\n",
    "    lemm_list=[token.lemma_ for token in doc]\n",
    "    return \" \".join(lemm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_tweet(self, text):\n",
    "        i = self.clearPunct(self.removeDigits(self.clean_text(self.removeEmoji(self.remove_NE(text))).lower()))\n",
    "        return self.clearSpaces(self.removeStop(self.lemmatize(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_tweet(tweet):\n",
    "    tweet=tweet.lower()\n",
    "    tweet=remove_User(tweet)\n",
    "    tweet=remove_Urls(tweet)\n",
    "    tweet=removeDigits(tweet)\n",
    "    tweet=clearPunct(tweet)\n",
    "    tweet=remove_stopwords(tweet)\n",
    "    tweet=lemmatise(tweet)\n",
    "    tweet=clear_spaces(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Forest fire near La Ronge Sask. Canada'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pd.read_csv('train.csv')['text'])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7608</td>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7609</td>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7610</td>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7611</td>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7612</td>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forest fire near la ronge sask canada'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tweet(list(pd.read_csv('train.csv')['text'])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [02:06<00:00, 60.41it/s] \n"
     ]
    }
   ],
   "source": [
    "train['prep_text'] = train['text'].progress_apply(lambda x: pre_tweet(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW+models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'prep_train' does not exist: b'prep_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8dd4bd2504db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'prep_train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'prep_train' does not exist: b'prep_train'"
     ]
    }
   ],
   "source": [
    "pd.read_csv('prep_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tweet('Err:509')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['prep_text']\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "X_train_=count_vectorizer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to generate bag of words\n",
    "count_vectorizer = feature_extraction.text.CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = count_vectorizer.fit_transform(X_train)\n",
    "test_vectors = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 9467)\n",
      "(2284, 9467)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.shape)\n",
    "print(test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=False)\n",
      "0.7819614711033275\n",
      "[[1053  225]\n",
      " [ 273  733]]\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "0.7841506129597198\n",
      "[[1168  110]\n",
      " [ 383  623]]\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)\n",
      "0.5626094570928196\n",
      "[[1278    0]\n",
      " [ 999    7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "0.7837127845884413\n",
      "[[1096  182]\n",
      " [ 312  694]]\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=1.0, loss='deviance', max_depth=1,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=0, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "0.7447460595446584\n",
      "[[1045  233]\n",
      " [ 350  656]]\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "                     weights='uniform')\n",
      "0.6970227670753065\n",
      "[[1182   96]\n",
      " [ 596  410]]\n",
      "[LibSVM]SVC(C=0.025, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=True)\n",
      "0.7552539404553416\n",
      "[[1212   66]\n",
      " [ 493  513]]\n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=2, kernel='rbf', max_iter=-1,\n",
      "    probability=False, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.62784588441331\n",
      "[[1269    9]\n",
      " [ 841  165]]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "0.6160245183887916\n",
      "[[1245   33]\n",
      " [ 844  162]]\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=5, max_features=1, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "0.5595446584938704\n",
      "[[1278    0]\n",
      " [1006    0]]\n",
      "MLPClassifier(activation='relu', alpha=1, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='adam', tol=0.001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "0.7810858143607706\n",
      "[[1086  192]\n",
      " [ 308  698]]\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "0.7250437828371279\n",
      "[[1089  189]\n",
      " [ 439  567]]\n"
     ]
    }
   ],
   "source": [
    "models=[MultinomialNB(fit_prior=False),\n",
    "SVC(gamma='scale'),\n",
    "RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), #\n",
    "LogisticRegression(random_state=0), \n",
    "GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0),\n",
    "KNeighborsClassifier(3),\n",
    "SVC(kernel=\"linear\", C=0.025,verbose=True),\n",
    "SVC(gamma=2, C=1),\n",
    "#GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "DecisionTreeClassifier(max_depth=5),\n",
    "RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "MLPClassifier(alpha=1, max_iter=200,tol=0.001),\n",
    "AdaBoostClassifier()]\n",
    "for i in models:\n",
    "    model=i.fit(train_vectors, y_train)\n",
    "    y_pred = model.predict(test_vectors)\n",
    "    print(str(i))\n",
    "    print(f1_score(y_test, y_pred, average='micro'))\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF+model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_vectors)\n",
    "test_tfidf = tfidf_transformer.fit_transform(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "0.7885288966725044\n",
      "[[1151  127]\n",
      " [ 356  650]]\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "0.780647985989492\n",
      "[[1151  127]\n",
      " [ 374  632]]\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)\n",
      "0.5678633975481612\n",
      "[[1278    0]\n",
      " [ 987   19]]\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "0.7793345008756567\n",
      "[[1073  205]\n",
      " [ 299  707]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "0.7828371278458844\n",
      "[[1129  149]\n",
      " [ 347  659]]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "0.7158493870402802\n",
      "[[953 325]\n",
      " [324 682]]\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)\n",
      "0.5683012259194395\n",
      "[[1277    1]\n",
      " [ 985   21]]\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=1.0, loss='deviance', max_depth=1,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=0, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "0.7504378283712785\n",
      "[[1053  225]\n",
      " [ 345  661]]\n"
     ]
    }
   ],
   "source": [
    "for i in models:\n",
    "    model=i.fit(train_tfidf, y_train)\n",
    "    y_pred = model.predict(test_tfidf)\n",
    "    print(str(i))\n",
    "    print(f1_score(y_test, y_pred, average='micro'))\n",
    "    print(f1_score(y_test, y_pred, average='micro'))\n",
    "\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In order to try the test ... but this is after the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "gt_df = pd.read_csv(\"socialmedia-disaster-tweets-DFE.csv\")\n",
    "\n",
    "\n",
    "gt_df = gt_df[['choose_one', 'text']]\n",
    "gt_df['target'] = (gt_df['choose_one']=='Relevant').astype(int)\n",
    "gt_df['id'] = gt_df.index\n",
    "gt_df\n",
    "\n",
    "merged_df = pd.merge(test_df, gt_df, on='id')\n",
    "merged_df\n",
    "\n",
    "subm_df = merged_df[['id', 'target']]\n",
    "subm_df\n",
    "\n",
    "subm_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "real=list(subm_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['prep_text']\n",
    "y = train['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=122)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = count_vectorizer.fit_transform(X)\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3263/3263 [00:32<00:00, 101.87it/s]\n"
     ]
    }
   ],
   "source": [
    "s=pd.read_csv('test.csv')\n",
    "s['target']=real\n",
    "s['prep_text']=s['text'].progress_apply(lambda x: pre_tweet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = count_vectorizer.transform(s['prep_text'])\n",
    "test_tfidf = tfidf_transformer.fit_transform(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=MultinomialNB().fit(train_tfidf,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = c.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7925222188170395"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(s['target'], y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7925222188170395"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred,s['target'], average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ....\n",
    "### let s start a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 1, 'b': 2},\n",
       " {'a': 1, 'b': 2},\n",
       " {'a': 2, 'b': 2},\n",
       " {'a': 2, 'b': 2},\n",
       " {'a': 3, 'b': 2},\n",
       " {'a': 3, 'b': 2},\n",
       " {'a': 3, 'b': 2},\n",
       " {'a': 3, 'b': 2}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_comb={'alpha':[0.005,0.25,0.5,0.75,1,1.25,1.5,1.75,2,2.25,2.5,2.75,3,3.25,3.5,3.75,4,4.25,4.5],\n",
    "            'fit_prior':[True,False]} ##combination of hyperparameters\n",
    "\n",
    "def MultinomialNB_grid_search(hyper_comb,X_train,y_train):\n",
    "    start=time.time()\n",
    "    all_res_cv = []\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=1848773) # 5 fold\n",
    "    grid = ParameterGrid(hyper_comb) #all possible combination of parameters chosen\n",
    "    for hypers in grid:\n",
    "        val_f1_list = []\n",
    "        train_f1_list = []\n",
    "        val_recall_list=[]\n",
    "        train_recall_list=[]\n",
    "        val_accuracy_list=[]\n",
    "        train_accuracy_list=[]\n",
    "        val_pre_list=[]\n",
    "        train_pre_list=[]\n",
    "        alpha = hypers['alpha']\n",
    "        fit_prior = hypers['fit_prior']\n",
    "\n",
    "        for train_index, test_index in cv.split(X_train): # cross validation\n",
    "            #take 80% and 20%\n",
    "            X_train_cv, X_val, y_train_cv, y_val = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
    "            #fit the model with the chosen parameters\n",
    "            model=MultinomialNB(alpha=alpha,fit_prior=fit_prior).fit(X_train_cv,y_train_cv)\n",
    "            # make prediction and compute F1\n",
    "            pred_train = model.predict(X_train_cv)\n",
    "            pred_val = model.predict(X_val)\n",
    "            #F1 SCORE\n",
    "            f1_train = f1_score(y_train_cv, pred_train, average='micro')\n",
    "            train_f1_list.append(f1_train)\n",
    "            f1_val = f1_score(y_val, pred_val, average='micro')\n",
    "            val_f1_list.append(f1_val)\n",
    "            #RECALL VALUES\n",
    "            recall_train=recall_score(y_train_cv,pred_train, average='micro')\n",
    "            train_recall_list.append(recall_train)\n",
    "            recall_val=recall_score(y_val,pred_val, average='micro')\n",
    "            val_recall_list.append(recall_val)\n",
    "            #ACCURACY\n",
    "            accuracy_train=accuracy_score(y_train_cv,pred_train)\n",
    "            train_accuracy_list.append(accuracy_train)\n",
    "            accuracy_val=accuracy_score(y_val,pred_val)\n",
    "            val_accuracy_list.append(accuracy_val)\n",
    "            #PRECISION\n",
    "            pre_train=precision_score(y_train_cv,pred_train, average='micro')\n",
    "            train_pre_list.append(pre_train)\n",
    "            pre_val=precision_score(y_val,pred_val, average='micro')\n",
    "            val_pre_list.append(pre_val)\n",
    "            \n",
    "        all_res_cv.append({'alpha':alpha,\n",
    "                           'fit_prior':fit_prior,\n",
    "                           'val_f1':np.mean(val_f1_list), \n",
    "                           'train_f1':np.mean(train_f1_list),\n",
    "                           'val_recall':np.mean(val_recall_list), \n",
    "                           'train_recall':np.mean(train_recall_list),\n",
    "                           'val_acc':np.mean(val_accuracy_list), \n",
    "                           'train_acc':np.mean(train_accuracy_list),\n",
    "                           'val_pre':np.mean(val_pre_list),\n",
    "                          'train_pre':np.mean(train_pre_list)})\n",
    "        print(({'alpha':alpha,'fit_prior':fit_prior,'val_f1':np.mean(val_f1_list), 'train_f1':np.mean(train_f1_list)}))\n",
    "\n",
    "        # after the 5th iteration we make the mean of the collected errors to find a more reliable value for the error\n",
    "    end=time.time()-start\n",
    "    df = pd.DataFrame(all_res_cv) # collect all results in a dataframe\n",
    "    df.to_csv('multinomial_grid_search_cv.csv', index=False) # save dataframe in csv with results\n",
    "\n",
    "    return print('File created grid_search_cv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.005, 'fit_prior': True, 'val_f1': 0.7726249198664714, 'train_f1': 0.9443386255367754}\n",
      "{'alpha': 0.005, 'fit_prior': False, 'val_f1': 0.7642182979681675, 'train_f1': 0.9424338905904133}\n",
      "{'alpha': 0.25, 'fit_prior': True, 'val_f1': 0.7902272038985231, 'train_f1': 0.9220412199322858}\n",
      "{'alpha': 0.25, 'fit_prior': False, 'val_f1': 0.7818209271244336, 'train_f1': 0.919742482582852}\n",
      "{'alpha': 0.5, 'fit_prior': True, 'val_f1': 0.7952183902888948, 'train_f1': 0.9118940890743268}\n",
      "{'alpha': 0.5, 'fit_prior': False, 'val_f1': 0.7877309204549083, 'train_f1': 0.9088729582719018}\n",
      "{'alpha': 0.75, 'fit_prior': True, 'val_f1': 0.7981081153370613, 'train_f1': 0.9039800141208097}\n",
      "{'alpha': 0.75, 'fit_prior': False, 'val_f1': 0.7919343608256406, 'train_f1': 0.9017141336689114}\n",
      "{'alpha': 1, 'fit_prior': True, 'val_f1': 0.7988962927619687, 'train_f1': 0.8971495751760585}\n",
      "{'alpha': 1, 'fit_prior': False, 'val_f1': 0.7942983754140412, 'train_f1': 0.8954748169457266}\n",
      "{'alpha': 1.25, 'fit_prior': True, 'val_f1': 0.7999471097141251, 'train_f1': 0.8926178681890613}\n",
      "{'alpha': 1.25, 'fit_prior': False, 'val_f1': 0.7946928523912362, 'train_f1': 0.8903519769538033}\n",
      "{'alpha': 1.5, 'fit_prior': True, 'val_f1': 0.8003409827239448, 'train_f1': 0.8880533043045284}\n",
      "{'alpha': 1.5, 'fit_prior': False, 'val_f1': 0.7944299540208265, 'train_f1': 0.8861158149025495}\n",
      "{'alpha': 1.75, 'fit_prior': True, 'val_f1': 0.8006034496890863, 'train_f1': 0.8848022938363125}\n",
      "{'alpha': 1.75, 'fit_prior': False, 'val_f1': 0.7948242584359142, 'train_f1': 0.8825036265787176}\n",
      "{'alpha': 2, 'fit_prior': True, 'val_f1': 0.8009977541041741, 'train_f1': 0.8822408954070706}\n",
      "{'alpha': 2, 'fit_prior': False, 'val_f1': 0.7937733552027044, 'train_f1': 0.8790555394254465}\n",
      "{'alpha': 2.25, 'fit_prior': True, 'val_f1': 0.8003412415671056, 'train_f1': 0.8799749987801324}\n",
      "{'alpha': 2.25, 'fit_prior': False, 'val_f1': 0.7933790507876166, 'train_f1': 0.8766911907228598}\n",
      "{'alpha': 2.5, 'fit_prior': True, 'val_f1': 0.8008668657458179, 'train_f1': 0.8774135787841708}\n",
      "{'alpha': 2.5, 'fit_prior': False, 'val_f1': 0.7935107156754555, 'train_f1': 0.8747208498150251}\n",
      "{'alpha': 2.75, 'fit_prior': True, 'val_f1': 0.8009982717904958, 'train_f1': 0.8752133905606241}\n",
      "{'alpha': 2.75, 'fit_prior': False, 'val_f1': 0.7945611875033973, 'train_f1': 0.8724549747548066}\n",
      "{'alpha': 3, 'fit_prior': True, 'val_f1': 0.8008666931837105, 'train_f1': 0.8726191514088864}\n",
      "{'alpha': 3, 'fit_prior': False, 'val_f1': 0.7948239995927534, 'train_f1': 0.8704190063187793}\n",
      "{'alpha': 3.25, 'fit_prior': True, 'val_f1': 0.8016547843275644, 'train_f1': 0.8704189685770197}\n",
      "{'alpha': 3.25, 'fit_prior': False, 'val_f1': 0.7944299540208266, 'train_f1': 0.8685144007727356}\n",
      "{'alpha': 3.5, 'fit_prior': True, 'val_f1': 0.801654611765457, 'train_f1': 0.8682516426426886}\n",
      "{'alpha': 3.5, 'fit_prior': False, 'val_f1': 0.7950867254010558, 'train_f1': 0.8663470586633647}\n",
      "{'alpha': 3.75, 'fit_prior': True, 'val_f1': 0.8013921448003154, 'train_f1': 0.8660514867692217}\n",
      "{'alpha': 3.75, 'fit_prior': False, 'val_f1': 0.7949554056374316, 'train_f1': 0.864508032120394}\n",
      "{'alpha': 4, 'fit_prior': True, 'val_f1': 0.8003409827239446, 'train_f1': 0.8645409213680095}\n",
      "{'alpha': 4, 'fit_prior': False, 'val_f1': 0.7950866391200023, 'train_f1': 0.8626033726575508}\n",
      "{'alpha': 4.25, 'fit_prior': True, 'val_f1': 0.8006037085322472, 'train_f1': 0.8628004547342858}\n",
      "{'alpha': 4.25, 'fit_prior': False, 'val_f1': 0.7946925072670218, 'train_f1': 0.8609942904805308}\n",
      "{'alpha': 4.5, 'fit_prior': True, 'val_f1': 0.8007347694527105, 'train_f1': 0.8612242186714415}\n",
      "{'alpha': 4.5, 'fit_prior': False, 'val_f1': 0.7939043298421143, 'train_f1': 0.8595822095050465}\n",
      "File created grid_search_cv\n"
     ]
    }
   ],
   "source": [
    "MultinomialNB_grid_search(hyper_comb,X_train_,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "     max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "     tol=0.001, verbose=False),)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC(gamma='scale'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "hyper_comb={'C':[0.005,0.25,0.5,0.75,1,1.25,1.5,1.75,2,2.25,2.5,2.75,3,3.25,3.5,3.75,4,4.25,4.5],\n",
    "            'coef0':[0.005,0.25,0.5,0.75,1,1.25,1.5,1.75,2,2.25,2.5,2.75,3,3.25,3.5,3.75,4,4.25,4.5],\n",
    "              'kernel':['linear','poly','rbf','sigmoid'],\n",
    "           'degree':[1,2,3,4,5],\n",
    "           'gamma':['scale','auto',0.005,0.25,0.5,0.75,1,1.25,1.5,1.75,2,2.25,2.5,2.75,3],\n",
    "           'shrinking':[True,False],\n",
    "           'decision_function_shape':['ovo','ovr']} ##combination of hyperparameters\n",
    "'''\n",
    "\n",
    "hyper_comb={'C':[0.005],\n",
    "            'coef0':[0.005,0.25],\n",
    "              'kernel':['linear','poly','rbf','sigmoid'],\n",
    "           'degree':[1,2,3],\n",
    "           'gamma':['scale','auto'],\n",
    "           'shrinking':[True,False],\n",
    "           'decision_function_shape':['ovo','ovr']} ##combination of hyperparameters\n",
    "def SVC_grid_search(hyper_comb,X_train,y_train):\n",
    "    start=time.time()\n",
    "    all_res_cv = []\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=1848773) # 5 fold\n",
    "    grid = ParameterGrid(hyper_comb) #all possible combination of parameters chosen\n",
    "    for hypers in grid:\n",
    "        val_f1_list = []\n",
    "        train_f1_list = []\n",
    "        val_recall_list=[]\n",
    "        train_recall_list=[]\n",
    "        val_accuracy_list=[]\n",
    "        train_accuracy_list=[]\n",
    "        val_pre_list=[]\n",
    "        train_pre_list=[]\n",
    "        C = hypers['C']\n",
    "        coef0 = hypers['coef0']\n",
    "        kernel=hypers['kernel']\n",
    "        degree=hypers['degree']\n",
    "        gamma=hypers['gamma']\n",
    "        shrinking=hypers['shrinking']\n",
    "        decision_function_shape=hypers['decision_function_shape']\n",
    "\n",
    "        for train_index, test_index in cv.split(X_train): # cross validation\n",
    "            #take 80% and 20%\n",
    "            X_train_cv, X_val, y_train_cv, y_val = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
    "            #fit the model with the chosen parameters\n",
    "            model=SVC(C=C,coef0=coef0,kernel=kernel,degree=degree,gamma=gamma,shrinking=shrinking,decision_function_shape=decision_function_shape,random_state=1848773).fit(X_train_cv,y_train_cv)\n",
    "            # make prediction and compute F1\n",
    "            pred_train = model.predict(X_train_cv)\n",
    "            pred_val = model.predict(X_val)\n",
    "            #F1 SCORE\n",
    "            f1_train = f1_score(y_train_cv, pred_train, average='micro')\n",
    "            train_f1_list.append(f1_train)\n",
    "            f1_val = f1_score(y_val, pred_val, average='micro')\n",
    "            val_f1_list.append(f1_val)\n",
    "            #RECALL VALUES\n",
    "            recall_train=recall_score(y_train_cv,pred_train, average='micro')\n",
    "            train_recall_list.append(recall_train)\n",
    "            recall_val=recall_score(y_val,pred_val, average='micro')\n",
    "            val_recall_list.append(recall_val)\n",
    "            #ACCURACY\n",
    "            accuracy_train=accuracy_score(y_train_cv,pred_train)\n",
    "            train_accuracy_list.append(accuracy_train)\n",
    "            accuracy_val=accuracy_score(y_val,pred_val)\n",
    "            val_accuracy_list.append(accuracy_val)\n",
    "            #PRECISION\n",
    "            pre_train=precision_score(y_train_cv,pred_train, average='micro')\n",
    "            train_pre_list.append(pre_train)\n",
    "            pre_val=precision_score(y_val,pred_val, average='micro')\n",
    "            val_pre_list.append(pre_val)\n",
    "            \n",
    "        all_res_cv.append({'C':C,\n",
    "                           'coef0' :coef0,\n",
    "                            'kernel':kernel,\n",
    "                            'degree':degree,\n",
    "                            'gamma':gamma,\n",
    "                        'shrinking':shrinking,\n",
    "                            'decision_function_shape':decision_function_shape,\n",
    "\n",
    "                           'val_f1':np.mean(val_f1_list), \n",
    "                           'train_f1':np.mean(train_f1_list),\n",
    "                           'val_recall':np.mean(val_recall_list), \n",
    "                           'train_recall':np.mean(train_recall_list),\n",
    "                           'val_acc':np.mean(val_accuracy_list), \n",
    "                           'train_acc':np.mean(train_accuracy_list),\n",
    "                           'val_pre':np.mean(val_pre_list),\n",
    "                          'train_pre':np.mean(train_pre_list)})\n",
    "        print(({'C':C,\n",
    "                           'coef0' :coef0,\n",
    "                            'kernel':kernel,\n",
    "                            'degree':degree,\n",
    "                            'gamma':gamma,\n",
    "                        'shrinking':shrinking,\n",
    "                            'decision_function_shape':decision_function_shape,'val_f1':np.mean(val_f1_list), 'train_f1':np.mean(train_f1_list)}))\n",
    "\n",
    "        # after the 5th iteration we make the mean of the collected errors to find a more reliable value for the error\n",
    "    end=time.time()-start\n",
    "    df = pd.DataFrame(all_res_cv) # collect all results in a dataframe\n",
    "    df.to_csv('SVC_grid_search_cv.csv', index=False) # save dataframe in csv with results\n",
    "\n",
    "    return print('File created grid_search_cv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SVC_grid_search(hyper_comb,X_train_,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_comb={'C':[0.005],\n",
    "            'dual':[True,False],\n",
    "              'fit_intercept':[True,False],\n",
    "          'intercept_scaling':[0.5,1,1.5],\n",
    "            'l1_ratio':[0.5],\n",
    "           'max_iter':[500],\n",
    "           'multi_class':['auto','ovr','multinomial'],\n",
    "            'penalty':['l1','l2','elasticnet','none'],\n",
    "           'warm_start':[True,False],\n",
    "           'random_state':[1848773],\n",
    "           'solver':['newton-cg','lbfgs','liblinear','sag','saga']}\n",
    "\n",
    "def log_grid_search(hyper_comb,X_train,y_train):\n",
    "    start=time.time()\n",
    "    all_res_cv = []\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=1848773) # 5 fold\n",
    "    grid = ParameterGrid(hyper_comb) #all possible combination of parameters chosen\n",
    "    for hypers in grid:\n",
    "        print(hypers)\n",
    "        C = hypers['C']\n",
    "        dual = hypers['dual']\n",
    "        fit_intercept=hypers['fit_intercept']\n",
    "        intercept_scaling=hypers['intercept_scaling']\n",
    "        l1_ratio=hypers['l1_ratio']\n",
    "        max_iter=hypers['max_iter']\n",
    "        multi_class=hypers['multi_class']\n",
    "        penalty=hypers['penalty']\n",
    "        warm_start=hypers['warm_start']\n",
    "        random_state=hypers['random_state']\n",
    "        solver=hypers['solver']\n",
    "        val_f1_list = []\n",
    "        train_f1_list = []\n",
    "        val_recall_list=[]\n",
    "        train_recall_list=[]\n",
    "        val_accuracy_list=[]\n",
    "        train_accuracy_list=[]\n",
    "        val_pre_list=[]\n",
    "        train_pre_list=[]\n",
    "        \n",
    "\n",
    "        for train_index, test_index in cv.split(X_train): # cross validation\n",
    "            try:\n",
    "                #take 80% and 20%\n",
    "                X_train_cv, X_val, y_train_cv, y_val = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
    "                #fit the model with the chosen parameters\n",
    "                model=LogisticRegression(C=C, class_weight=None, dual=dual, fit_intercept=fit_intercept,\n",
    "                       intercept_scaling=intercept_scaling, l1_ratio=l1_ratio, max_iter=max_iter,\n",
    "                       multi_class=multi_class, n_jobs=None, penalty=penalty,\n",
    "                       random_state=random_state, solver=solver, tol=0.0001,\n",
    "                       warm_start=warm_start).fit(X_train_cv,y_train_cv)\n",
    "                # make prediction and compute F1\n",
    "                pred_train = model.predict(X_train_cv)\n",
    "                pred_val = model.predict(X_val)\n",
    "                #F1 SCORE\n",
    "                f1_train = f1_score(y_train_cv, pred_train, average='micro')\n",
    "                train_f1_list.append(f1_train)\n",
    "                f1_val = f1_score(y_val, pred_val, average='micro')\n",
    "                val_f1_list.append(f1_val)\n",
    "                #RECALL VALUES\n",
    "                recall_train=recall_score(y_train_cv,pred_train, average='micro')\n",
    "                train_recall_list.append(recall_train)\n",
    "                recall_val=recall_score(y_val,pred_val, average='micro')\n",
    "                val_recall_list.append(recall_val)\n",
    "                #ACCURACY\n",
    "                accuracy_train=accuracy_score(y_train_cv,pred_train)\n",
    "                train_accuracy_list.append(accuracy_train)\n",
    "                accuracy_val=accuracy_score(y_val,pred_val)\n",
    "                val_accuracy_list.append(accuracy_val)\n",
    "                #PRECISION\n",
    "                pre_train=precision_score(y_train_cv,pred_train, average='micro')\n",
    "                train_pre_list.append(pre_train)\n",
    "                pre_val=precision_score(y_val,pred_val, average='micro')\n",
    "                val_pre_list.append(pre_val)\n",
    "\n",
    "\n",
    "\n",
    "                all_res_cv.append({'C':C,\n",
    "                                   'dual' :dual,\n",
    "                                    'fit_intercept':fit_intercept,\n",
    "                                    'intercept_scaling':intercept_scaling,\n",
    "                                    'l1_ratio':l1_ratio,\n",
    "                                'max_iter':max_iter,\n",
    "                                    'multi_class':decision_function_shape,\n",
    "                                   'penalty':penalty,\n",
    "                                   'warm_start':warm_start,\n",
    "                                   'random_state':random_state,\n",
    "                                   'solver':solver,\n",
    "                                   'val_f1':np.mean(val_f1_list), \n",
    "                                   'train_f1':np.mean(train_f1_list),\n",
    "                                   'val_recall':np.mean(val_recall_list), \n",
    "                                   'train_recall':np.mean(train_recall_list),\n",
    "                                   'val_acc':np.mean(val_accuracy_list), \n",
    "                                   'train_acc':np.mean(train_accuracy_list),\n",
    "                                   'val_pre':np.mean(val_pre_list),\n",
    "                                  'train_pre':np.mean(train_pre_list)})\n",
    "                print(({'C':C,\n",
    "                                   'dual' :dual,\n",
    "                                    'fit_intercept':fit_intercept,\n",
    "                                    'intercept_scaling':intercept_scaling,\n",
    "                                    'l1_ratio':l1_ratio,\n",
    "                                'max_iter':max_iter,\n",
    "                                    'multi_class':decision_function_shape,\n",
    "                                   'penalty':penalty,\n",
    "                                   'warm_start':warm_start,\n",
    "                                   'random_state':random_state,\n",
    "                                   'solver':solver,'val_f1':np.mean(val_f1_list), 'train_f1':np.mean(train_f1_list)}))\n",
    "            except:\n",
    "                print('excep')\n",
    "                pass\n",
    "\n",
    "        # after the 5th iteration we make the mean of the collected errors to find a more reliable value for the error\n",
    "    end=time.time()-start\n",
    "    df = pd.DataFrame(all_res_cv) # collect all results in a dataframe\n",
    "    df.to_csv('log_grid_search_cv.csv', index=False) # save dataframe in csv with results\n",
    "\n",
    "    return print('File created grid_search_cv')\n",
    "\n",
    "##combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPClassifier(activation='relu', alpha=1, batch_size='auto', beta_1=0.9,\n",
    "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
    "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
    "              random_state=None, shuffle=True, solver='adam', tol=0.001,\n",
    "              validation_fraction=0.1, verbose=False, warm_start=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_comb={'activation':['identity','logistic','tanh','relu'],\n",
    "'alpha':[0.001,0.005,0.01],\n",
    "'batch_size':['auto',300,400],\n",
    "'beta_1':[0.4,0.5,0.9],\n",
    "'beta_2':[0,0.5],\n",
    "'early_stopping':[True,False],\n",
    "'learning_rate_init':[0.001,0.004,0.008],\n",
    "'max_iter':[100,200,300],\n",
    "'momentum':[0,0.1,0.3,0.5,0.7],\n",
    "'n_iter_no_change':[10,20],\n",
    "'nesterovs_momentum':[True,False],\n",
    "'power_t':[0.5,0.3]\n",
    "'random_state':[1848773],\n",
    "'shuffle':[True,False],\n",
    "'solver':['lbfgs','sgd','adam'],\n",
    "'tol':[1e-3],\n",
    "'validation_fraction':[0.1,0.3,0.5]}\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_comb={'activation':['identity','logistic','tanh','relu'],\n",
    "'alpha':[0.001,0.005,0.01],\n",
    "'batch_size':['auto',300,400],\n",
    "'beta_1':[0.4,0.5,0.9],\n",
    "'beta_2':[0,0.5],\n",
    "'early_stopping':[True,False],\n",
    "'learning_rate_init':[0.001,0.004,0.008],\n",
    "'max_iter':[100,200,300],\n",
    "'momentum':[0,0.1,0.3,0.5,0.7],\n",
    "'n_iter_no_change':[10,20],\n",
    "'nesterovs_momentum':[True,False],\n",
    "'power_t':[0.5,0.3],\n",
    "'random_state':[1848773],\n",
    "'shuffle':[True,False],\n",
    "'solver':['lbfgs','sgd','adam'],\n",
    "'tol':[1e-3],\n",
    "'validation_fraction':[0.1,0.3,0.5],\n",
    "'hidden_layer_sizes':[(100,)],\n",
    "'learning_rate':[0.001],\n",
    "'warm_start':[True]}\n",
    "\n",
    "def mlp_grid_search(hyper_comb,X_train,y_train):\n",
    "    start=time.time()\n",
    "    all_res_cv = []\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=1848773) # 5 fold\n",
    "    grid = ParameterGrid(hyper_comb) #all possible combination of parameters chosen\n",
    "    for hypers in grid:\n",
    "        activation=hypers['activation']\n",
    "        alpha=hypers['alpha']\n",
    "        batch_size=hypers['batch_size']\n",
    "        beta_1=hypers['beta_1']\n",
    "        beta_2=hypers['beta_2']\n",
    "        early_stopping=hypers['early_stopping']\n",
    "        learning_rate_init=hypers['learning_rate_init']\n",
    "        learning_rate=hypers['learning_rate']\n",
    "        max_iter=hypers['max_iter']\n",
    "        momentum=hypers['momentum']\n",
    "        warm_start=hypers['warm_start']\n",
    "        n_iter_no_change=hypers['n_iter_no_change']\n",
    "        nesterovs_momentum=hypers['nesterovs_momentum']\n",
    "        power_t=hypers['power_t']\n",
    "        random_state=hypers['random_state']\n",
    "        shuffle=hypers['shuffle']\n",
    "        solver=hypers['solver']\n",
    "        tol=hypers['tol']\n",
    "        validation_fraction=hypers['validation_fraction']\n",
    "        hidden_layer_sizes=hypers['hidden_layer_sizes']\n",
    "        val_f1_list = []\n",
    "        train_f1_list = []\n",
    "        val_recall_list=[]\n",
    "        train_recall_list=[]\n",
    "        val_accuracy_list=[]\n",
    "        train_accuracy_list=[]\n",
    "        val_pre_list=[]\n",
    "        train_pre_list=[]\n",
    "        \n",
    "\n",
    "        for train_index, test_index in cv.split(X_train): # cross validation\n",
    "            \n",
    "            #take 80% and 20%\n",
    "            X_train_cv, X_val, y_train_cv, y_val = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
    "            #fit the model with the chosen parameters\n",
    "            model=MLPClassifier(activation=activation, alpha=alpha, batch_size=batch_size, beta_1=beta_1,\n",
    "              beta_2=beta_2, early_stopping=early_stopping, epsilon=1e-08,\n",
    "              hidden_layer_sizes=hidden_layer_sizes, learning_rate=learning_rate,\n",
    "              learning_rate_init=learning_rate_init, max_iter=max_iter, momentum=momentum,\n",
    "              n_iter_no_change=n_iter_no_changes, nesterovs_momentum=nesterovs_momentum, power_t=power_t,\n",
    "              random_state=random_state, shuffle=shuffle, solver=solver, tol=tol,\n",
    "              validation_fraction=validation_fraction, verbose=False, warm_start=warm_start).fit(X_train_cv,y_train_cv)\n",
    "\n",
    "            # make prediction and compute F1\n",
    "            pred_train = model.predict(X_train_cv)\n",
    "            pred_val = model.predict(X_val)\n",
    "            #F1 SCORE\n",
    "            f1_train = f1_score(y_train_cv, pred_train, average='micro')\n",
    "            train_f1_list.append(f1_train)\n",
    "            f1_val = f1_score(y_val, pred_val, average='micro')\n",
    "            val_f1_list.append(f1_val)\n",
    "            #RECALL VALUES\n",
    "            recall_train=recall_score(y_train_cv,pred_train, average='micro')\n",
    "            train_recall_list.append(recall_train)\n",
    "            recall_val=recall_score(y_val,pred_val, average='micro')\n",
    "            val_recall_list.append(recall_val)\n",
    "            #ACCURACY\n",
    "            accuracy_train=accuracy_score(y_train_cv,pred_train)\n",
    "            train_accuracy_list.append(accuracy_train)\n",
    "            accuracy_val=accuracy_score(y_val,pred_val)\n",
    "            val_accuracy_list.append(accuracy_val)\n",
    "            #PRECISION\n",
    "            pre_train=precision_score(y_train_cv,pred_train, average='micro')\n",
    "            train_pre_list.append(pre_train)\n",
    "            pre_val=precision_score(y_val,pred_val, average='micro')\n",
    "            val_pre_list.append(pre_val)\n",
    "\n",
    "\n",
    "\n",
    "            all_res_cv.append({'activation':activation,\n",
    "                                'alpha':alpha,\n",
    "                                'batch_size':batch_size,\n",
    "                                'beta_1':beta_1,\n",
    "                                'beta_2':beta_2,\n",
    "                                'early_stopping':early_stopping,\n",
    "                                'learning_rate_init':learning_rate_init,\n",
    "                                'max_iter':max_iter,\n",
    "                                'momentum':momentum,\n",
    "                                'n_iter_no_change':n_iter_no_change,\n",
    "                                'nesterovs_momentum':nesterovs_momentum,\n",
    "                                'power_t':power_t,\n",
    "                                'random_state':random_state,\n",
    "                                'shuffle':shuffle,\n",
    "                                'solver':solver,\n",
    "                                'tol':tol,\n",
    "                                'validation_fraction':validation_fraction,\n",
    "                                'hidden_layer_sizes':hidden_layer_sizes,\n",
    "                                'learning_rate':learning_rate,\n",
    "                                'warm_start':warm_start,\n",
    "                               'val_f1':np.mean(val_f1_list), \n",
    "                               'train_f1':np.mean(train_f1_list),\n",
    "                               'val_recall':np.mean(val_recall_list), \n",
    "                               'train_recall':np.mean(train_recall_list),\n",
    "                               'val_acc':np.mean(val_accuracy_list), \n",
    "                               'train_acc':np.mean(train_accuracy_list),\n",
    "                               'val_pre':np.mean(val_pre_list),\n",
    "                              'train_pre':np.mean(train_pre_list)})\n",
    "            print(({'activation':activation,\n",
    "                                'alpha':alpha,\n",
    "                                'batch_size':batch_size,\n",
    "                                'beta_1':beta_1,\n",
    "                                'beta_2':beta_2,\n",
    "                                'early_stopping':early_stopping,\n",
    "                                'learning_rate_init':learning_rate_init,\n",
    "                                'max_iter':max_iter,\n",
    "                                'momentum':momentum,\n",
    "                                'n_iter_no_change':n_iter_no_change,\n",
    "                                'nesterovs_momentum':nesterovs_momentum,\n",
    "                                'power_t':power_t,\n",
    "                                'random_state':random_state,\n",
    "                                'shuffle':shuffle,\n",
    "                                'solver':solver,\n",
    "                                'tol':tol,\n",
    "                                'validation_fraction':validation_fraction,\n",
    "                                'hidden_layer_sizes':hidden_layer_sizes,\n",
    "                                'learning_rate':learning_rate,\n",
    "                                'warm_start':warm_start,\n",
    "                'val_f1':np.mean(val_f1_list), 'train_f1':np.mean(train_f1_list)}))\n",
    "\n",
    "\n",
    "        # after the 5th iteration we make the mean of the collected errors to find a more reliable value for the error\n",
    "    end=time.time()-start\n",
    "    df = pd.DataFrame(all_res_cv) # collect all results in a dataframe\n",
    "    df.to_csv('mlp_grid_search_cv.csv', index=False) # save dataframe in csv with results\n",
    "\n",
    "    return print('File created grid_search_cv')\n",
    "\n",
    "##combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,n_estimators=50, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_comb={ 'learning_rate':[0.5,1.0,2.0],\n",
    "            'n_estimators':[50,60,70]}\n",
    "#THE HYPERPARAMETER BASE_ESTIMATOR CAN BE CHANGED BUT FOR SIMPLICITY\n",
    "def ada_grid_search(hyper_comb,X_train,y_train):\n",
    "    start=time.time()\n",
    "    all_res_cv = []\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=1848773) # 5 fold\n",
    "    grid = ParameterGrid(hyper_comb) #all possible combination of parameters chosen\n",
    "    for hypers in grid:\n",
    "        learning_rate=hypers['learning_rate']\n",
    "        n_estimators=hypers['n_estimators']\n",
    "        val_f1_list = []\n",
    "        train_f1_list = []\n",
    "        val_recall_list=[]\n",
    "        train_recall_list=[]\n",
    "        val_accuracy_list=[]\n",
    "        train_accuracy_list=[]\n",
    "        val_pre_list=[]\n",
    "        train_pre_list=[]\n",
    "        \n",
    "\n",
    "        for train_index, test_index in cv.split(X_train): # cross validation\n",
    "            \n",
    "            #take 80% and 20%\n",
    "            X_train_cv, X_val, y_train_cv, y_val = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
    "            #fit the model with the chosen parameters\n",
    "            model=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=learning_rate,n_estimators=n_estimators, random_state=1848773).fit(X_train_cv,y_train_cv)\n",
    "\n",
    "            # make prediction and compute F1\n",
    "            pred_train = model.predict(X_train_cv)\n",
    "            pred_val = model.predict(X_val)\n",
    "            #F1 SCORE\n",
    "            f1_train = f1_score(y_train_cv, pred_train, average='micro')\n",
    "            train_f1_list.append(f1_train)\n",
    "            f1_val = f1_score(y_val, pred_val, average='micro')\n",
    "            val_f1_list.append(f1_val)\n",
    "            #RECALL VALUES\n",
    "            recall_train=recall_score(y_train_cv,pred_train, average='micro')\n",
    "            train_recall_list.append(recall_train)\n",
    "            recall_val=recall_score(y_val,pred_val, average='micro')\n",
    "            val_recall_list.append(recall_val)\n",
    "            #ACCURACY\n",
    "            accuracy_train=accuracy_score(y_train_cv,pred_train)\n",
    "            train_accuracy_list.append(accuracy_train)\n",
    "            accuracy_val=accuracy_score(y_val,pred_val)\n",
    "            val_accuracy_list.append(accuracy_val)\n",
    "            #PRECISION\n",
    "            pre_train=precision_score(y_train_cv,pred_train, average='micro')\n",
    "            train_pre_list.append(pre_train)\n",
    "            pre_val=precision_score(y_val,pred_val, average='micro')\n",
    "            val_pre_list.append(pre_val)\n",
    "\n",
    "\n",
    "\n",
    "            all_res_cv.append({'learning_rate':learning_rate,\n",
    "                                'n_estimators':n_estimators,\n",
    "                               'val_f1':np.mean(val_f1_list), \n",
    "                               'train_f1':np.mean(train_f1_list),\n",
    "                               'val_recall':np.mean(val_recall_list), \n",
    "                               'train_recall':np.mean(train_recall_list),\n",
    "                               'val_acc':np.mean(val_accuracy_list), \n",
    "                               'train_acc':np.mean(train_accuracy_list),\n",
    "                               'val_pre':np.mean(val_pre_list),\n",
    "                              'train_pre':np.mean(train_pre_list)})\n",
    "            print(({'learning_rate':learning_rate,\n",
    "                                'n_estimators':n_estimators,\n",
    "                'val_f1':np.mean(val_f1_list), 'train_f1':np.mean(train_f1_list)}))\n",
    "\n",
    "\n",
    "        # after the 5th iteration we make the mean of the collected errors to find a more reliable value for the error\n",
    "    end=time.time()-start\n",
    "    df = pd.DataFrame(all_res_cv) # collect all results in a dataframe\n",
    "    df.to_csv('ada_grid_search_cv.csv', index=False) # save dataframe in csv with results\n",
    "\n",
    "    return print('File created grid_search_cv')\n",
    "\n",
    "##combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
    "                     weights='uniform')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['algorithm', 'leaf_size', 'metric', 'n_neighbors', 'p', 'weighths'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_comb={ 'algorithm':['auto','ball_tree','kd_tree','brute'],\n",
    "             'leaf_size':[30],\n",
    "             'metric':['minkowski'],\n",
    "             'n_neighbors':[2,3,4],\n",
    "            'p':[1,2,3],\n",
    "           'weighths':['uniform','distance']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_comb={ 'algorithm':['auto','ball_tree','kd_tree','brute'],\n",
    "             'leaf_size':[30],\n",
    "             'metric':['minkowski'],\n",
    "             'n_neighbors':[2,3,4],\n",
    "            'p':[1,2,3],\n",
    "           'weighths':['uniform','distance']}\n",
    "\n",
    "#THE HYPERPARAMETER BASE_ESTIMATOR CAN BE CHANGED BUT FOR SIMPLICITY\n",
    "def KNC_grid_search(hyper_comb,X_train,y_train):\n",
    "    start=time.time()\n",
    "    all_res_cv = []\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=1848773) # 5 fold\n",
    "    grid = ParameterGrid(hyper_comb) #all possible combination of parameters chosen\n",
    "    for hypers in grid:\n",
    "        algorithm=hypers['algorithm']\n",
    "        leaf_size=hypers['leaf_size']\n",
    "        metric=hypers['metric']\n",
    "        n_neighbors=hypers['n_neighbors']\n",
    "        p=hypers['p']\n",
    "        weights=hypers['weights']\n",
    "        val_f1_list = []\n",
    "        train_f1_list = []\n",
    "        val_recall_list=[]\n",
    "        train_recall_list=[]\n",
    "        val_accuracy_list=[]\n",
    "        train_accuracy_list=[]\n",
    "        val_pre_list=[]\n",
    "        train_pre_list=[]\n",
    "        \n",
    "\n",
    "        for train_index, test_index in cv.split(X_train): # cross validation\n",
    "            \n",
    "            #take 80% and 20%\n",
    "            X_train_cv, X_val, y_train_cv, y_val = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
    "            #fit the model with the chosen parameters\n",
    "            model=KNeighborsClassifier(algorithm=algorithm, leaf_size=leaf_size, metric=metric, n_jobs=None, n_neighbors=n_neighbors, p=p,\n",
    "                     weights=weights).fit(X_train_cv,y_train_cv)\n",
    "\n",
    "            # make prediction and compute F1\n",
    "            pred_train = model.predict(X_train_cv)\n",
    "            pred_val = model.predict(X_val)\n",
    "            #F1 SCORE\n",
    "            f1_train = f1_score(y_train_cv, pred_train, average='micro')\n",
    "            train_f1_list.append(f1_train)\n",
    "            f1_val = f1_score(y_val, pred_val, average='micro')\n",
    "            val_f1_list.append(f1_val)\n",
    "            #RECALL VALUES\n",
    "            recall_train=recall_score(y_train_cv,pred_train, average='micro')\n",
    "            train_recall_list.append(recall_train)\n",
    "            recall_val=recall_score(y_val,pred_val, average='micro')\n",
    "            val_recall_list.append(recall_val)\n",
    "            #ACCURACY\n",
    "            accuracy_train=accuracy_score(y_train_cv,pred_train)\n",
    "            train_accuracy_list.append(accuracy_train)\n",
    "            accuracy_val=accuracy_score(y_val,pred_val)\n",
    "            val_accuracy_list.append(accuracy_val)\n",
    "            #PRECISION\n",
    "            pre_train=precision_score(y_train_cv,pred_train, average='micro')\n",
    "            train_pre_list.append(pre_train)\n",
    "            pre_val=precision_score(y_val,pred_val, average='micro')\n",
    "            val_pre_list.append(pre_val)\n",
    "\n",
    "\n",
    "\n",
    "            all_res_cv.append({ 'algorithm':algorithm,\n",
    "                                 'leaf_size':leaf_size,\n",
    "                                 'metric':metric,\n",
    "                                 'n_neighbors':n_neighbors,\n",
    "                                    'p':p,\n",
    "                                   'weighths':weights,\n",
    "                               'val_f1':np.mean(val_f1_list), \n",
    "                               'train_f1':np.mean(train_f1_list),\n",
    "                               'val_recall':np.mean(val_recall_list), \n",
    "                               'train_recall':np.mean(train_recall_list),\n",
    "                               'val_acc':np.mean(val_accuracy_list), \n",
    "                               'train_acc':np.mean(train_accuracy_list),\n",
    "                               'val_pre':np.mean(val_pre_list),\n",
    "                              'train_pre':np.mean(train_pre_list)})\n",
    "            print(({'algorithm':algorithm,\n",
    "                                 'leaf_size':leaf_size,\n",
    "                                 'metric':metric,\n",
    "                                 'n_neighbors':n_neighbors,\n",
    "                                    'p':p,\n",
    "                                   'weighths':weights,\n",
    "                'val_f1':np.mean(val_f1_list), 'train_f1':np.mean(train_f1_list)}))\n",
    "\n",
    "\n",
    "        # after the 5th iteration we make the mean of the collected errors to find a more reliable value for the error\n",
    "    end=time.time()-start\n",
    "    df = pd.DataFrame(all_res_cv) # collect all results in a dataframe\n",
    "    df.to_csv('knc_grid_search_cv.csv', index=False) # save dataframe in csv with results\n",
    "\n",
    "    return print('File created grid_search_cv')\n",
    "\n",
    "##combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "                           learning_rate=1.0, loss='deviance', max_depth=1,\n",
    "                           max_features=None, max_leaf_nodes=None,\n",
    "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                           min_samples_leaf=1, min_samples_split=2,\n",
    "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                           n_iter_no_change=None, presort='auto',\n",
    "                           random_state=0, subsample=1.0, tol=0.0001,\n",
    "                           validation_fraction=0.1, verbose=0,\n",
    "                           warm_start=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_comb={'criterion':['friedman_mse'],\n",
    "            'init':[None,'zero'],\n",
    "           'learning_rate':[0.1,0.4],\n",
    "            'loss':['deviance','exponential'],\n",
    "            'max_depth':[1,2],\n",
    "            'max_features':['auto','sqrt','log2',None],\n",
    "            'max_leaf_nodes':[None],\n",
    "            'min_impurity_decrease':[0],\n",
    "            'min_impurity_split':[None],\n",
    "            'min_samples_leaf':[1],\n",
    "            'min_samples_split':[2],\n",
    "            'min_weight_fraction_leaf':[0.0],\n",
    "            'n_estimators':[100],\n",
    "            'n_iter_no_change':[None],\n",
    "            'presort':['deprecated'],\n",
    "            'subsample':[1,2,3],\n",
    "            'tol':[0.001],\n",
    "            'validation_fraction':[0.1],\n",
    "            'warm_start':[True,False]}\n",
    "\n",
    "#THE HYPERPARAMETER BASE_ESTIMATOR CAN BE CHANGED BUT FOR SIMPLICITY\n",
    "def GBC_grid_search(hyper_comb,X_train,y_train):\n",
    "    start=time.time()\n",
    "    all_res_cv = []\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=1848773) # 5 fold\n",
    "    grid = ParameterGrid(hyper_comb) #all possible combination of parameters chosen\n",
    "    for hypers in grid:\n",
    "        criterion=hypers['criterion']\n",
    "        init=hypers['init']\n",
    "        learning_rate=hypers['learning_rate']\n",
    "        loss=hypers['loss']\n",
    "        max_depth=hypers['max_depth']\n",
    "        max_features=hypers['max_features']\n",
    "        max_leaf_nodes=hypers['max_leaf_nodes']\n",
    "        min_impurity=hypers['min_impurity_decrease']\n",
    "        min_impurity_split=hypers['min_impurity_split']\n",
    "        min_samples_leaf=hypers['min_samples_leaf']\n",
    "        min_samples_split=hypers['min_samples_split']\n",
    "        min_weight_fraction_leaf=hypers['min_weight_fraction_leaf']\n",
    "        n_estimators= hypers['n_estimators']\n",
    "        n_iter_no_change=hypers['n_iter_no_change']\n",
    "        presort=hypers['presort']\n",
    "        subsample=hypers['subsample']\n",
    "        tol=hypers['tol']\n",
    "        validation_fraction=hypers['validation_fraction']\n",
    "        warm_start=hypers['warm_start']\n",
    "        val_f1_list = []\n",
    "        train_f1_list = []\n",
    "        val_recall_list=[]\n",
    "        train_recall_list=[]\n",
    "        val_accuracy_list=[]\n",
    "        train_accuracy_list=[]\n",
    "        val_pre_list=[]\n",
    "        train_pre_list=[]\n",
    "        \n",
    "\n",
    "        for train_index, test_index in cv.split(X_train): # cross validation\n",
    "            \n",
    "            #take 80% and 20%\n",
    "            X_train_cv, X_val, y_train_cv, y_val = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
    "            #fit the model with the chosen param1848773eters\n",
    "            model=GradientBoostingClassifier(criterion=criterion, init=init,\n",
    "                           learning_rate=learning_rate, loss=loss, max_depth=max_depth,\n",
    "                           max_features=max_features, max_leaf_nodes=max_leaf_nodes,\n",
    "                           min_impurity_decrease=min_impurity_decrease, min_impurity_split=min_impurity_split,\n",
    "                           min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split,\n",
    "                           min_weight_fraction_leaf=min_weight_fraction_leaf, n_estimators=n_estimators,\n",
    "                           n_iter_no_change=n_iter_no_change, presort=presort,\n",
    "                           random_state=, subsample=subsample, tol=tol,\n",
    "                           validation_fraction=validation_fraction, verbose=0,\n",
    "                           warm_start=warn_start).fit(X_train_cv,y_train_cv)\n",
    "\n",
    "            # make prediction and compute F1\n",
    "            pred_train = model.predict(X_train_cv)\n",
    "            pred_val = model.predict(X_val)\n",
    "            #F1 SCORE\n",
    "            f1_train = f1_score(y_train_cv, pred_train, average='micro')\n",
    "            train_f1_list.append(f1_train)\n",
    "            f1_val = f1_score(y_val, pred_val, average='micro')\n",
    "            val_f1_list.append(f1_val)\n",
    "            #RECALL VALUES\n",
    "            recall_train=recall_score(y_train_cv,pred_train, average='micro')\n",
    "            train_recall_list.append(recall_train)\n",
    "            recall_val=recall_score(y_val,pred_val, average='micro')\n",
    "            val_recall_list.append(recall_val)\n",
    "            #ACCURACY\n",
    "            accuracy_train=accuracy_score(y_train_cv,pred_train)\n",
    "            train_accuracy_list.append(accuracy_train)\n",
    "            accuracy_val=accuracy_score(y_val,pred_val)\n",
    "            val_accuracy_list.append(accuracy_val)\n",
    "            #PRECISION\n",
    "            pre_train=precision_score(y_train_cv,pred_train, average='micro')\n",
    "            train_pre_list.append(pre_train)\n",
    "            pre_val=precision_score(y_val,pred_val, average='micro')\n",
    "            val_pre_list.append(pre_val)\n",
    "\n",
    "\n",
    "\n",
    "        all_res_cv.append({ 'criterion':criterion,\n",
    "        'init':init,\n",
    "        'learning_rate':learning_rate,\n",
    "        'loss':loss,\n",
    "        'max_depth':max_depth,\n",
    "        'max_features':max_features,\n",
    "        'max_leaf_nodes':max_leaf_nodes,\n",
    "        'min_impurity':min_impurity_decrease,\n",
    "        'min_impurity_split':min_impurity_split,\n",
    "        'min_samples_leaf':min_samples_leaf,\n",
    "        'min_samples_split':min_samples_split,\n",
    "        'min_weight_fraction_leaf':min_weight_fraction_leaf,\n",
    "        'n_estimators':n_estimators,\n",
    "        'n_iter_no_change':n_iter_no_change,\n",
    "        'presort':presort,\n",
    "        'subsample':subsample,\n",
    "        'tol':tol,\n",
    "        'validation_fraction':validation_fraction,\n",
    "        'warm_start':warm_start,\n",
    "        'val_f1':np.mean(val_f1_list), \n",
    "        'train_f1':np.mean(train_f1_list),\n",
    "        'val_recall':np.mean(val_recall_list), \n",
    "        'train_recall':np.mean(train_recall_list),\n",
    "        'val_acc':np.mean(val_accuracy_list), \n",
    "        'train_acc':np.mean(train_accuracy_list),\n",
    "        'val_pre':np.mean(val_pre_list),\n",
    "        'train_pre':np.mean(train_pre_list)})\n",
    "        print(({'criterion':criterion,\n",
    "        'init':init,\n",
    "        'learning_rate':learning_rate,\n",
    "        'loss':loss,\n",
    "        'max_depth':max_depth,\n",
    "        'max_features':max_features,\n",
    "        'max_leaf_nodes':max_leaf_nodes,\n",
    "        'min_impurity':min_impurity_decrease,\n",
    "        'min_impurity_split':min_impurity_split,\n",
    "        'min_samples_leaf':min_samples_leaf,\n",
    "        'min_samples_split':min_samples_split,\n",
    "        'min_weight_fraction_leaf':min_weight_fraction_leaf,\n",
    "        'n_estimators':n_estimators,\n",
    "        'n_iter_no_change':n_iter_no_change,\n",
    "        'presort':presort,\n",
    "        'subsample':subsample,\n",
    "        'tol':tol,\n",
    "        'validation_fraction':validation_fraction,\n",
    "        'warm_start':warm_start,\n",
    "            'val_f1':np.mean(val_f1_list), 'train_f1':np.mean(train_f1_list)}))\n",
    "\n",
    "\n",
    "        # after the 5th iteration we make the mean of the collected errors to find a more reliable value for the error\n",
    "    end=time.time()-start\n",
    "    df = pd.DataFrame(all_res_cv) # collect all results in a dataframe\n",
    "    df.to_csv('gbc_grid_search_cv.csv', index=False) # save dataframe in csv with results\n",
    "\n",
    "    return print('File created grid_search_cv')\n",
    "\n",
    "##combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "                       max_features=None, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort=False,\n",
    "                       random_state=None, splitter='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_comb={'class_weight':[None,'balanced'],\n",
    "            'criterion':['gini'],\n",
    "            'max_depth':[None],\n",
    "            'max_features':[None],\n",
    "            'max_leaf_nodes':[None],\n",
    "            'min_impurity_decrease':[0],\n",
    "            'min_impurity_split':[None],\n",
    "            'min_samples_leaf':[1],\n",
    "            'min_samples_split':[2],\n",
    "            'min_weight_fraction_leaf':[0.0],\n",
    "            'presort':[False],\n",
    "            'splitter':['best']}\n",
    "    \n",
    "def DT_grid_search(hyper_comb,X_train,y_train):\n",
    "    start=time.time()\n",
    "    all_res_cv = []\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=1848773) # 5 fold\n",
    "    grid = ParameterGrid(hyper_comb) #all possible combination of parameters chosen\n",
    "    for hypers in grid:\n",
    "        class_weight=hypers['class_weight'],\n",
    "        criterion=hypers['criterion']\n",
    "        max_depth=hypers['max_depth']\n",
    "        max_features=hypers['max_features']\n",
    "        max_leaf_nodes=hypers['max_leaf_nodes']\n",
    "        min_impurity_decrease=hypers['min_impurity_decrease']\n",
    "        min_impurity_split=hypers['min_impurity_split']\n",
    "        min_samples_leaf=hypers['min_samples_leaf']\n",
    "        min_samples_split=hypers['min_samples_split']\n",
    "        min_weight_fraction_leaf=hypers['min_weigth_fraction_leaf']\n",
    "        presort=hypers['presort']\n",
    "        splitter=hypers['splitter']\n",
    "        val_f1_list = []\n",
    "        train_f1_list = []\n",
    "        val_recall_list=[]\n",
    "        train_recall_list=[]\n",
    "        val_accuracy_list=[]\n",
    "        train_accuracy_list=[]\n",
    "        val_pre_list=[]\n",
    "        train_pre_list=[]\n",
    "        \n",
    "\n",
    "        for train_index, test_index in cv.split(X_train): # cross validation\n",
    "            \n",
    "            #take 80% and 20%\n",
    "            X_train_cv, X_val, y_train_cv, y_val = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
    "            #fit the model with the chosen param1848773eters\n",
    "            model=DecisionTreeClassifier(class_weight=class_weight, criterion=criterion, max_depth=max_depth,\n",
    "                       max_features=max_features, max_leaf_nodes=max_leaf_nodes,\n",
    "                       min_impurity_decrease=min_impurity_decrease, min_impurity_split=min_impurity_split,\n",
    "                       min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split,\n",
    "                       min_weight_fraction_leaf=min_weight_fraction_leaf, presort=presort,\n",
    "                       random_state=1848773, splitter=splitter).fit(X_train_cv,y_train_cv)\n",
    "\n",
    "            # make prediction and compute F1\n",
    "            pred_train = model.predict(X_train_cv)\n",
    "            pred_val = model.predict(X_val)\n",
    "            #F1 SCORE\n",
    "            f1_train = f1_score(y_train_cv, pred_train, average='micro')\n",
    "            train_f1_list.append(f1_train)\n",
    "            f1_val = f1_score(y_val, pred_val, average='micro')\n",
    "            val_f1_list.append(f1_val)\n",
    "            #RECALL VALUES\n",
    "            recall_train=recall_score(y_train_cv,pred_train, average='micro')\n",
    "            train_recall_list.append(recall_train)\n",
    "            recall_val=recall_score(y_val,pred_val, average='micro')\n",
    "            val_recall_list.append(recall_val)\n",
    "            #ACCURACY\n",
    "            accuracy_train=accuracy_score(y_train_cv,pred_train)\n",
    "            train_accuracy_list.append(accuracy_train)\n",
    "            accuracy_val=accuracy_score(y_val,pred_val)\n",
    "            val_accuracy_list.append(accuracy_val)\n",
    "            #PRECISION\n",
    "            pre_train=precision_score(y_train_cv,pred_train, average='micro')\n",
    "            train_pre_list.append(pre_train)\n",
    "            pre_val=precision_score(y_val,pred_val, average='micro')\n",
    "            val_pre_list.append(pre_val)\n",
    "\n",
    "\n",
    "\n",
    "        all_res_cv.append({ 'class_weight':class_weight,\n",
    "            'criterion':criterion,\n",
    "            'max_depth':max_depth,\n",
    "            'max_features':max_features,\n",
    "            'max_leaf_nodes':max_leaf_nodes,\n",
    "            'min_impurity_decrease':min_impurity_decrease,\n",
    "            'min_impurity_split':min_impurity_split,\n",
    "            'min_samples_leaf':min_samples_leaf,\n",
    "            'min_samples_split':min_samples_split,\n",
    "            'min_weight_fraction_leaf':min_weight_fraction_leaf,\n",
    "            'presort':presort,\n",
    "            'splitter':splitter,\n",
    "        'val_f1':np.mean(val_f1_list), \n",
    "        'train_f1':np.mean(train_f1_list),\n",
    "        'val_recall':np.mean(val_recall_list), \n",
    "        'train_recall':np.mean(train_recall_list),\n",
    "        'val_acc':np.mean(val_accuracy_list), \n",
    "        'train_acc':np.mean(train_accuracy_list),\n",
    "        'val_pre':np.mean(val_pre_list),\n",
    "        'train_pre':np.mean(train_pre_list)})\n",
    "        print(({'class_weight':class_weight,\n",
    "            'criterion':criterion,\n",
    "            'max_depth':max_depth,\n",
    "            'max_features':max_features,\n",
    "            'max_leaf_nodes':max_leaf_nodes,\n",
    "            'min_impurity_decrease':min_impurity_decrease,\n",
    "            'min_impurity_split':min_impurity_split,\n",
    "            'min_samples_leaf':min_samples_leaf,\n",
    "            'min_samples_split':min_samples_split,\n",
    "            'min_weight_fraction_leaf':min_weight_fraction_leaf,\n",
    "            'presort':presort,\n",
    "            'splitter':splitter,\n",
    "            'val_f1':np.mean(val_f1_list), 'train_f1':np.mean(train_f1_list)}))\n",
    "\n",
    "\n",
    "        # after the 5th iteration we make the mean of the collected errors to find a more reliable value for the error\n",
    "    end=time.time()-start\n",
    "    df = pd.DataFrame(all_res_cv) # collect all results in a dataframe\n",
    "    df.to_csv('DT_grid_search_cv.csv', index=False) # save dataframe in csv with results\n",
    "\n",
    "    return print('File created grid_search_cv')\n",
    "\n",
    "##combination of hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
